# Install kaggle (only for kaggle to import the data directly)
%pip install torch
! pip install -q kaggle

# Import Api Tokan file
from google.colab import files
files.upload()

# Import and unzip dataset
!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d kazanova/sentiment140
!mkdir train
! unzip sentiment140.zip -d train
import torch
from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

# Read the CSV
df=pd.read_csv('/content/train/training.1600000.processed.noemoticon.csv', encoding='latin1', 
               names=['target', 'ids', 'date', 'flag', 'user', 'text'])
df.head()
set(df.target)
set(df.flag)

# we can drop unnessary datasets which will not affected on the results.
df.drop(['flag','date','ids','user'],inplace=True, axis=1)
df.head()

# Preprocess the data
label_to_target={0:'negative',2:'neutral',4:'positive'}
def label_decoder(label):
  return label_to_target[label]
df.target = df.target.apply(lambda x: label_decoder(x))
df.head()
df.target.value_counts()
#Cleaning the data
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words=stopwords.words('english')
print(stop_words)

# Split the dataset using train_test_split
from sklearn.model_selection import train_test_split
train_data, test_data=train_test_split(df, test_size=0.2, random_state=50)
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.tokenize import TweetTokenizer

# Use tweet tokanizer
tweet_tokenizer = TweetTokenizer()

x_train_tokens = train_data['text'].apply(lambda x: tweet_tokenizer.tokenize(x))
x_test_tokens = test_data['text'].apply(lambda x: tweet_tokenizer.tokenize(x))

tokenizer = Tokenizer(num_words=5000)

tokenizer.fit_on_texts(x_train_tokens)

x_train_seq = tokenizer.texts_to_sequences(x_train_tokens)
x_test_seq = tokenizer.texts_to_sequences(x_test_tokens)

# for having same length
max_length = 100
x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post')
x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post')

vocab_size = len(tokenizer.word_index) + 1



vocab_size
type(x_train_pad)
x_train_pad.shape
from tensorflow.keras.utils import to_categorical

# Convert the target labels to numerical values
label_to_target = {'negative': 0, 'neutral': 1, 'positive': 2}
train_labels = train_data['target'].map(label_to_target)
test_labels = test_data['target'].map(label_to_target)

# Convert the labels to one-hot encoded format
num_classes = 3
y_train = to_categorical(train_labels, num_classes=num_classes)
y_test = to_categorical(test_labels, num_classes=num_classes)


y_train.shape
y_test.shape

# Create the model
max_length=100
embedding_dim = 100
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,LSTM,Dense

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))
model.add(LSTM(128))
model.add(Dense(3, activation='softmax'))

# compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#train the model
history=model.fit(x_train_pad, y_train, epochs=20, batch_size=1000,validation_data = (x_test_pad,y_test))

# Plot the training and validation loss
import matplotlib.pyplot as plt
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
loss, accuracy = model.evaluate(x_test_pad, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

# Test the model using sample tweet
label_to_target = {0: 'negative', 1: 'neutral', 2: 'positive'}

sample_tweet = 'On his birth anniversary, we pay homage to the former President of India, Neelam Sanjiva Reddy, 
                one of the youngest to hold the highest office in the country.An activist during the freedom movement, 
                he also served as the Chief Minister of Andhra Pradesh and was instrumental in modernising the state. 
                Today we honour his contributions to strengthening Indias democracy.'
                
sample_tweet_tokens = tweet_tokenizer.tokenize(sample_tweet)
sample_tweet_seq = tokenizer.texts_to_sequences([sample_tweet_tokens])
sample_tweet_pad = pad_sequences(sample_tweet_seq, maxlen=max_length, padding='post')
predicted_sentiment = label_to_target[np.argmax(model.predict(sample_tweet_pad))]
print("Predicted Sentiment:", predicted_sentiment)

# Save the model
model.save('Sentiment_analysis_on_1.6_million tweets.h5')
